import ssm
import numpy as np
from IPython import embed
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import ray
import argparse


def get_args():
    """Sets the command line arguments to run this script"""
    parser = argparse.ArgumentParser(description="global variables to set input file names")
    parser.add_argument("-i", "--input", help="inputs file name", required=True, type=str)
    parser.add_argument("-c", "--choice", help="choices file name", required=True, type=str)
    parser.add_argument("-e", "--exp", help="experiment name to use for figure title", required=True, type=str)
    parser.add_argument("-s", "--state", help="the number of states to use", required=True, type=int)
    return parser.parse_args()

ray.init()

@ray.remote
def train_hmm(num_states: int, inputs_train, inputs_test, choices_train, choices_test):
    """Takes number of states, trainig datasets for input and choice data returns log likelyhood"""
    print(obs_dim)
    hmm = ssm.HMM(num_states, obs_dim, input_dim, observations="input_driven_obs", 
                   observation_kwargs=dict(C=num_categories), transitions="standard")
    train_ll = hmm.fit(np.concatenate(choices_train), inputs=np.concatenate(inputs_train), method="em", num_iters=N_iters, tolerance=TOL)

    log_likeli_test = hmm.log_likelihood(np.concatenate(choices_test), inputs=np.concatenate(inputs_test))
    log_likeli_train = hmm.log_likelihood(np.concatenate(choices_train), inputs=np.concatenate(inputs_train))
    return log_likeli_test, log_likeli_train


if __name__ == "__main__":
    args = get_args()
    choice_f: str = args.choice #holds path or file name of mouse choice data
    input_f: str = args.input #holds path or file name of mouse inputs data
    experiment: str = args.exp #holds experiment number to use as a graph header
    num_states: int =  args.state #holds number of states

     #load mouse inputs and choices
    inputs = np.load(input_f,allow_pickle = True)
    choices = np.load(choice_f,allow_pickle = True)

    obs_dim: int = choices[0].shape[1]          # number of observed dimensions
    num_categories: int = len(np.unique(np.concatenate(choices)))    # number of categories for output
    input_dim: int = inputs[0].shape[1]                                    # input dimensions

    TOL: int = 10**-4 # tolerance 
    N_iters: int = 1000 # number of iterations for the fitting model


    #split data into k number of folds and train/test the model
    nKfold=5 #split data into 5 training and 5 testing data sets
    log_likeli_test_dict = {new_list: [] for new_list in range(1, num_states+1)} #dictionary to hold test data likelihoods
    log_likeli_train_dict = {new_list: [] for new_list in range(1, num_states+1)} #dictionary to hold train data likelihoods
    kf = KFold(n_splits=nKfold, shuffle=True, random_state=None)
    # generate training and testing datasets for inputs and choices
    # kf.split returns indices in the given dataset
    input_train = list()
    input_test = list()
    ray_tasks = list()
    for k in kf.split(inputs):
        for i in range(num_states):
            train_ind = k[0]
            test_ind = k[1]
            # use indices generated by kf.split to access and save actual data points
            input_train = np.take(inputs, train_ind)
            input_test = np.take(inputs, test_ind)
            choice_train = np.take(choices, train_ind)
            choice_test = np.take(choices, test_ind)
            # call fitting and testing function
            ray_tasks.append(train_hmm.remote(i+1, input_train, input_test, choice_train, choice_test))
            #log_likeli_test_dict[i+1].append(likelyhood[0])
            #log_likeli_train_dict[i+1].append(likelyhood[1])

    results = ray.get(ray_tasks)
    #print(f'{results=}')
    #print(f'{log_likeli_test_dict=}')
    #print(f'{log_likeli_train_dict=}')

    for i in range(1, num_states+1):
        for k in range(5):
            log_likeli_test_dict[i].append(results[i][0])
            log_likeli_train_dict[i].append(results[i][1])

    
    print(f'{log_likeli_test_dict=}')
    print(f'{log_likeli_train_dict=}')

    ray.shutdown()

    #generate graph from the obtained likelyhood datapoints
    keys = list(log_likeli_test_dict.keys())
    values = list(log_likeli_test_dict.values())

    x1 = []
    y1 = []
    x2 = []
    y2 = []


    for key in keys:
        x1.extend([key] * len(log_likeli_test_dict[key]))
        y1.extend(log_likeli_test_dict[key])

        x2.extend([key] * len(log_likeli_train_dict[key]))
        y2.extend(log_likeli_train_dict[key])

    plt.scatter(x1, y1, label='test_data', color='blue')
    plt.scatter(x2, y2, label='train_data', color='red')
    plt.xlabel("State_n")
    plt.ylabel("Log_likelyhood")
    plt.title(f"Log_likelihood_{experiment}")
    plt.legend()
    plt.savefig(f'log_likelyhood_{experiment}.png') 