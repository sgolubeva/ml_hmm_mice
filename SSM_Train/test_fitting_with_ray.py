#!/usr/bin/env python3
import ssm
import numpy as np
#from IPython import embed
from sklearn.model_selection import KFold
import matplotlib.pyplot as plt
import ray
import argparse


def get_args():
    """Sets the command line arguments to run this script"""
    parser = argparse.ArgumentParser(description="global variables to set input file names")
    parser.add_argument("-i", "--input", help="inputs file name", required=True, type=str)
    parser.add_argument("-c", "--choice", help="choices file name", required=True, type=str)
    parser.add_argument("-e", "--exp", help="experiment name to use for figure title", required=True, type=str)
    parser.add_argument("-s", "--state", help="the number of states to use", required=True, type=int)
    return parser.parse_args()

ray.init()

@ray.remote
def train_hmm(num_states: int, inputs_train, inputs_test, choices_train, choices_test):
    """Takes number of states, trainig datasets for input and choice data returns log likelyhood"""
    print(obs_dim)
    hmm = ssm.HMM(num_states, obs_dim, input_dim, observations="input_driven_obs", 
                   observation_kwargs=dict(C=num_categories), transitions="standard")
    train_ll = hmm.fit(np.concatenate(choices_train), inputs=np.concatenate(inputs_train), method="em", num_iters=N_iters, tolerance=TOL)

    log_likeli_test = hmm.log_likelihood(np.concatenate(choices_test), inputs=np.concatenate(inputs_test))
    log_likeli_train = hmm.log_likelihood(np.concatenate(choices_train), inputs=np.concatenate(inputs_train))
    return log_likeli_test, log_likeli_train


if __name__ == "__main__":
    args = get_args()
    choice_f: str = args.choice #holds path or file name of mouse choice data
    input_f: str = args.input #holds path or file name of mouse inputs data
    experiment: str = args.exp #holds experiment number to use as a graph header
    num_states: int =  args.state #holds number of states

     #load mouse inputs and choices
    inputs = np.load(input_f,allow_pickle = True)
    choices = np.load(choice_f,allow_pickle = True)
    new_choices = list()
    new_inputs = list()
    for i in range(len(choices)):
        inds = np.squeeze(choices[i]!=2)
        new_inputs.append(inputs[i][inds,:])
        new_choices.append(choices[i][inds])
    
    new_choices = np.array(new_choices, dtype='O')
    new_inputs = np.array(new_inputs, dtype='O')

    obs_dim: int = choices[0].shape[1]          # number of observed dimensions
    num_categories: int = len(np.unique(np.concatenate(choices)))    # number of categories for output
    input_dim: int = inputs[0].shape[1]                                    # input dimensions

    TOL: int = 10**-4 # tolerance 
    N_iters: int = 1000 # number of iterations for the fitting model


    #split data into k number of folds and train/test the model
    nKfold=5 #split data into 5 training and 5 testing data sets
    log_likeli_test_dict = {new_list: [] for new_list in range(1, num_states+1)} #dictionary to hold test data likelihoods
    log_likeli_train_dict = {new_list: [] for new_list in range(1, num_states+1)} #dictionary to hold train data likelihoods
    kf = KFold(n_splits=nKfold, shuffle=True, random_state=None)
    # generate training and testing datasets for inputs and choices
    # kf.split returns indices in the given dataset
    input_train = list()
    input_test = list()
    ray_tasks = list()
    for k in kf.split(new_inputs):
        for i in range(num_states):
            train_ind = k[0]
            test_ind = k[1]
            # use indices generated by kf.split to access and save actual data points
            input_train = np.take(new_inputs, train_ind)
            input_test = np.take(new_inputs, test_ind)
            choice_train = np.take(new_choices, train_ind)
            choice_test = np.take(new_choices, test_ind)
            # call fitting and testing function
            ray_tasks.append(train_hmm.remote(i+1, input_train, input_test, choice_train, choice_test))


    results = ray.get(ray_tasks)

    count = 0
    state = 1
    for like in results:
        log_likeli_test_dict[state].append(like[0])
        log_likeli_train_dict[state].append(like[1])
        count+=1
        if count == 5:
            count = 0
            state+=1


    ray.shutdown()

    with open(f'likel_data_test_{experiment}.csv', 'w') as ts, open(f'lekel_data_train_{experiment}.csv', 'w') as tr:
        for state in log_likeli_test_dict:
            ts.write(f'{state},{log_likeli_test_dict[state][0]},{log_likeli_test_dict[state][1]},{log_likeli_test_dict[state][2]},{log_likeli_test_dict[state][3]},{log_likeli_test_dict[state][4]}\n')
        for state in log_likeli_train_dict:
            tr.write(f'{state},{log_likeli_train_dict[state][0]},{log_likeli_train_dict[state][1]},{log_likeli_train_dict[state][2]},{log_likeli_train_dict[state][3]},{log_likeli_train_dict[state][4]}\n') 
    #generate graph from the obtained likelyhood datapoints
#     keys = list(log_likeli_test_dict.keys())
#     values = list(log_likeli_train_dict.values())

# x1 = []
# y1 = []
# x2 = []
# y2 = []

# fig, axs = plt.subplots(2, 1)
# fig.text(0.5,0.04, "State number n", ha="center", va="center")
# fig.text(0.02,0.5, "Log Likelihood", ha="center", va="center", rotation=90)
# for key in keys:
#     x1.extend([key])
#     y1.append(np.mean(log_likeli_test_dict[key]))

#     x2.extend([key])
#     y2.append(np.mean(log_likeli_train_dict[key]))

# axs[0].scatter(x1, y1, label='test_data', color='blue')
# axs[1].scatter(x2, y2, label='train_data', color='red')
# axs[1].yaxis.tick_right()
# axs[1].yaxis.set_ticks_position('both')
# axs[0].yaxis.set_ticks_position('both')
# axs[0].set_title(f"Log_likelihood_{experiment}")

# plt.subplots_adjust(wspace=0, hspace=0)
# axs[0].legend(loc='lower right')
# axs[1].legend(loc='lower right')
# plt.savefig(f'log_likelihood_binary_{experiment}') 